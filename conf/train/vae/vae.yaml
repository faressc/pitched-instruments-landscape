epochs: 3000
batch_size: 256
lr: 1e-4 # learning rate
wd: 0.05 # weight decay
betas: [0.9, 0.95] # adam betas
dropout_ratio: 0.2
eval_interval: 50
visualize_interval: 300
hear_interval: 300
save_interval: 1000

# crop the time dimension to this length (150 -> input_crop)
input_crop: 150

# vae with convolutional layers
channels: [128, 256, 512, 1024, 2048]
linears: [8192, 4096, 2048, 1024, 512, 2]



# loss function parameter
rec_beta: 0.2 # reconstruction loss, comparing input audio embedding and generated embedding
rep_beta: 0.6 # repulsion loss for maximising the distance of in-batch samples
spa_beta: 1.0 # spatial loss for limiting samples to the unicircle area to be used as a user interface
cla_beta: 0. # classifier for estimating the pitch from the encoder head
inst_beta: 0.07 # instrument classification from 2d embedding (for globally arranging samples)
reg_scaling_exp: 2.0 # scaling exponent for the repultion loss (possible good values: 1.5, 2.0, 2.5, 3.0)




calculate_vae_loss:
  _target_: vae.calculate_vae_loss
  _partial_: true
  num_epochs: ${train.vae.epochs}
  weighted_reproduction: false
  loss_fn:
    _target_: torch.nn.MSELoss
    reduction: sum
  cls_loss_fn:
    _target_: torch.nn.CrossEntropyLoss
  batch_size: ${train.vae.batch_size}