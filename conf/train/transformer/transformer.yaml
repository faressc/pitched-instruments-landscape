epochs: 20
batch_size: 128
learning_rate: 1e-4  # max learning rate
weight_decay: 0.05
betas: [0.9, 0.95]  # adam betas
eval_interval: 5
visualize_interval: 5
hear_interval: 5
save_interval: 5
num_batches_evaluation: 5 # how many batches of samples should be generated in eval_model? Takes a lot of time!

# transformer config
block_size: 300
input_dimension: 128
internal_dimension: 1024
feedforward_dimension: 2048
n_layer_encoder: 8
n_layer_decoder: 12
n_head: 16
dropout: 0.0

condition_model_path: "out/vae/checkpoints/vae_final_epoch_200.torch"

loss_fn:
  _target_: torch.nn.MSELoss
  reduction: mean
