epochs: 5000
batch_size: 64
learning_rate: 1e-4  # max learning rate
weight_decay: 0.05
betas: [0.9, 0.95]  # adam betas
eval_interval: 100
visualize_interval: 500
hear_interval: 500
save_interval: 500
num_batches_evaluation: 5 # how many batches of samples should be generated in eval_model? Takes a lot of time!

# transformer config
block_size: 300
input_dimension: 128
internal_dimension: 512
feedforward_dimension: 1024
n_layer_encoder: 8
n_layer_decoder: 12
n_head: 8
dropout: 0.0

condition_model_path: "out/vae/checkpoints/vae_final_epoch_200.torch"

loss_fn:
  _target_: torch.nn.MSELoss
  reduction: mean
