epochs: 5000
batch_size: 64
learning_rate: 1e-4  # max learning rate
weight_decay: 0.05
betas: [0.9, 0.95]  # adam betas
eval_interval: 50
visualize_interval: 50
hear_interval: 500
save_interval: 1000
num_batches_evaluation: 20 # how many batches of samples should be generated in eval_model? Takes a lot of time!

# transformer config
block_size: 300
input_dimension: 128
internal_dimension: 512
feedforward_dimension: 4096
n_layer_encoder: 8
n_layer_decoder: 12
n_head: 8
dropout: 0.1

std_multiplier: 0.005

loss_fn:
  _target_: torch.nn.MSELoss
  reduction: mean
