schema: '2.0'
stages:
  preprocess:
    cmd: python -u source/preprocess.py
    deps:
    - path: data/raw/
      hash: md5
      md5: 25e54827bb56e0446b074ca8411aec84.dir
      size: 39391890330
      nfiles: 305982
    - path: source/preprocess.py
      hash: md5
      md5: 384ea4345776f9d6e3e5371e6eec5dea
      size: 17607
    params:
      params.yaml:
        preprocess:
          batch_size: 8
          db_writemap: false
          ext:
          - wav
          initial_db_size: 1
          input_path_test: data/raw/nsynth-test
          input_path_train: data/raw/nsynth-train
          input_path_valid: data/raw/nsynth-valid
          model_name: facebook/encodec_24khz
          num_workers: 30
          output_path_test: data/processed/test
          output_path_train: data/processed/train
          output_path_valid: data/processed/valid
          sample_rate: 24000
    outs:
    - path: data/processed/
      hash: md5
      md5: 6e310ab2456715138e33193d0554577d.dir
      size: 130354565120
      nfiles: 6
  train_vae:
    cmd: python -u source/train_vae.py
    deps:
    - path: source/dataset.py
      hash: md5
      md5: 3348427ec290bcac2ee111bb4732f4d9
      size: 8452
    - path: source/train_vae.py
      hash: md5
      md5: 1d24599d1195314a34de874259310cb3
      size: 25641
    - path: source/vae.py
      hash: md5
      md5: 2bf2d3d9aaee6e94768492b83dc552b8
      size: 13656
    params:
      params.yaml:
        train:
          vae:
            epochs: 15000
            batch_size: 256
            lr: 0.0003
            wd: 0.05
            betas:
            - 0.9
            - 0.95
            dropout_ratio: 0.1
            eval_interval: 50
            visualize_interval: 50
            hear_interval: 300
            save_interval: 1000
            input_crop: 150
            channels:
            - 128
            - 256
            - 512
            - 1024
            - 1024
            - 512
            linears:
            - 4096
            - 2048
            - 1024
            - 512
            - 2
            rec_beta: 0.8
            neighbor_beta: 1.0
            spa_beta: 0.1
            kl_beta: 0.0039
            note_cls_beta: 0.12
            family_cls_beta: 0.07
            instrument_cls_beta: 0.007
            reg_scaling_exp_neighbor: 0.5
            reg_scaling_exp_family: 1.5
            reg_scaling_exp_instrument: 0.8
            calculate_vae_loss:
              _target_: vae.calculate_vae_loss
              _partial_: true
              num_epochs: 15000
              weighted_reproduction: false
              loss_fn:
                _target_: torch.nn.MSELoss
                reduction: sum
              cls_loss_fn:
                _target_: torch.nn.CrossEntropyLoss
              batch_size: 256
          transformer:
            epochs: 5000
            batch_size: 64
            learning_rate: 0.0001
            weight_decay: 0.05
            betas:
            - 0.9
            - 0.95
            eval_interval: 50
            visualize_interval: 50
            hear_interval: 500
            save_interval: 1000
            num_batches_evaluation: 20
            block_size: 300
            input_dimension: 128
            internal_dimension: 512
            feedforward_dimension: 4096
            n_layer_encoder: 8
            n_layer_decoder: 12
            n_head: 8
            dropout: 0.1
            condition_model_path: out/vae/checkpoints/vae_final_epoch_10000.torch
            loss_fn:
              _target_: torch.nn.MSELoss
              reduction: mean
          db_path_train: data/processed/train
          db_path_valid: data/processed/valid
          db_path_test: data/processed/test
          random_seed: 42
          num_workers: 0
          pitch:
          - 48
          - 49
          - 50
          - 51
          - 52
          - 53
          - 54
          - 55
          - 56
          - 57
          - 58
          - 59
          - 60
          - 61
          - 62
          - 63
          - 64
          - 65
          - 66
          - 67
          - 68
          - 69
          - 70
          - 71
          - 72
          max_inst_per_family: 10
          velocity:
          - 100
          device: auto
          deterministic: true
          vae_path: out/vae/checkpoints/vae_final_epoch_10000.torch
          transformer_path: out/transformer/checkpoints/transformer_final_epoch_5000.torch
    outs:
    - path: out/vae/
      hash: md5
      md5: cca57b15fc4be8d7a161786cacb927b9.dir
      size: 4249298008
      nfiles: 30
  train_transformer:
    cmd: python -u source/train_transformer.py
    deps:
    - path: out/vae/
      hash: md5
      md5: cca57b15fc4be8d7a161786cacb927b9.dir
      size: 4249298008
      nfiles: 30
    - path: source/dataset.py
      hash: md5
      md5: 3348427ec290bcac2ee111bb4732f4d9
      size: 8452
    - path: source/train_transformer.py
      hash: md5
      md5: d6f3d232dc2c2fc4743f6b8c8d09dbff
      size: 16730
    - path: source/transformer.py
      hash: md5
      md5: 674f6cf0a163d446cce8a8687d9c0b2f
      size: 4117
    params:
      params.yaml:
        train:
          vae:
            epochs: 15000
            batch_size: 256
            lr: 0.0003
            wd: 0.05
            betas:
            - 0.9
            - 0.95
            dropout_ratio: 0.1
            eval_interval: 50
            visualize_interval: 50
            hear_interval: 300
            save_interval: 1000
            input_crop: 150
            channels:
            - 128
            - 256
            - 512
            - 1024
            - 1024
            - 512
            linears:
            - 4096
            - 2048
            - 1024
            - 512
            - 2
            rec_beta: 0.8
            neighbor_beta: 1.0
            spa_beta: 0.1
            kl_beta: 0.006
            note_cls_beta: 0.12
            family_cls_beta: 0.07
            instrument_cls_beta: 0.007
            reg_scaling_exp_neighbor: 0.5
            reg_scaling_exp_family: 1.5
            reg_scaling_exp_instrument: 0.8
            calculate_vae_loss:
              _target_: vae.calculate_vae_loss
              _partial_: true
              num_epochs: 15000
              weighted_reproduction: false
              loss_fn:
                _target_: torch.nn.MSELoss
                reduction: sum
              cls_loss_fn:
                _target_: torch.nn.CrossEntropyLoss
              batch_size: 256
          transformer:
            epochs: 5000
            batch_size: 64
            learning_rate: 0.0001
            weight_decay: 0.05
            betas:
            - 0.9
            - 0.95
            eval_interval: 50
            visualize_interval: 50
            hear_interval: 500
            save_interval: 1000
            num_batches_evaluation: 20
            block_size: 300
            input_dimension: 128
            internal_dimension: 512
            feedforward_dimension: 8192
            n_layer_encoder: 8
            n_layer_decoder: 12
            n_head: 8
            dropout: 0.1
            std_multiplier: 0.0
            loss_fn:
              _target_: torch.nn.MSELoss
              reduction: mean
          db_path_train: data/processed/train
          db_path_valid: data/processed/valid
          db_path_test: data/processed/test
          random_seed: 42
          num_workers: 25
          pitch:
          - 48
          - 49
          - 50
          - 51
          - 52
          - 53
          - 54
          - 55
          - 56
          - 57
          - 58
          - 59
          - 60
          - 61
          - 62
          - 63
          - 64
          - 65
          - 66
          - 67
          - 68
          - 69
          - 70
          - 71
          - 72
          max_inst_per_family: 10
          velocity:
          - 100
          device: auto
          deterministic: true
          vae_path: out/vae/checkpoints/vae_final_timid-orfe_epoch_15000.torch
          transformer_path: out/transformer/checkpoints/transformer_final_hazel-shay_epoch_5000.torch
    outs:
    - path: out/transformer/
      hash: md5
      md5: 97892e4bbdd6d5d3193d052ccc64ea7f.dir
      size: 4848371301
      nfiles: 12
